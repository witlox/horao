{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Validation of Research Datasets\n",
    "\n",
    "This notebook analyzes and validates the datasets generated by the research validation suites in:\n",
    "- Distributed Systems Consistency\n",
    "- DevOps Practices\n",
    "- Security Multi-Cloud\n",
    "- Telemetry and Observability\n",
    "\n",
    "We'll perform statistical analysis on the data to validate completeness, consistency, and identify any anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Set better default plot styling\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the research areas based on the repository structure\n",
    "RESEARCH_AREAS = [\n",
    "    'distributed_systems_consistency',\n",
    "    'devops_practices',\n",
    "    'security_multi_cloud',\n",
    "    'telemetry_observability'\n",
    "]\n",
    "\n",
    "# Function to get all test modules in a research area\n",
    "def get_test_modules(area):\n",
    "    area_dir = Path(area)\n",
    "    if not area_dir.exists():\n",
    "        return []\n",
    "    \n",
    "    return [f.stem for f in area_dir.glob('test_*.py')]\n",
    "\n",
    "# Analyze the test modules in each research area\n",
    "test_modules_by_area = {}\n",
    "for area in RESEARCH_AREAS:\n",
    "    test_modules_by_area[area] = get_test_modules(area)\n",
    "    \n",
    "print(\"Test modules found in research areas:\")\n",
    "for area, modules in test_modules_by_area.items():\n",
    "    print(f\"\\n{area.replace('_', ' ').title()}:\")\n",
    "    for module in modules:\n",
    "        print(f\"  - {module.replace('test_', '').replace('_', ' ').title()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Discovery and Loading\n",
    "\n",
    "First, let's explore what data files are available in the research/data directory and load them for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data directory path\n",
    "DATA_DIR = Path('data')\n",
    "\n",
    "# Function to scan and categorize data files\n",
    "def scan_data_files():\n",
    "    if not DATA_DIR.exists():\n",
    "        print(f\"Warning: Data directory {DATA_DIR} does not exist!\")\n",
    "        return {}\n",
    "    \n",
    "    # Get all data files recursively\n",
    "    all_files = list(DATA_DIR.glob('**/*'))\n",
    "    \n",
    "    # Filter out directories\n",
    "    data_files = [f for f in all_files if f.is_file()]\n",
    "    \n",
    "    # Categorize by file extension\n",
    "    files_by_extension = {}\n",
    "    for file_path in data_files:\n",
    "        ext = file_path.suffix.lower()\n",
    "        if ext not in files_by_extension:\n",
    "            files_by_extension[ext] = []\n",
    "        files_by_extension[ext].append(file_path)\n",
    "    \n",
    "    # Categorize by research area if possible\n",
    "    files_by_area = {area: [] for area in RESEARCH_AREAS}\n",
    "    files_by_area['unknown'] = []\n",
    "    \n",
    "    for file_path in data_files:\n",
    "        assigned = False\n",
    "        for area in RESEARCH_AREAS:\n",
    "            if area in str(file_path):\n",
    "                files_by_area[area].append(file_path)\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            files_by_area['unknown'].append(file_path)\n",
    "    \n",
    "    return {\n",
    "        'all_files': data_files,\n",
    "        'by_extension': files_by_extension,\n",
    "        'by_area': files_by_area\n",
    "    }\n",
    "\n",
    "data_files_info = scan_data_files()\n",
    "\n",
    "# Display summary of data files\n",
    "if data_files_info:\n",
    "    print(f\"Found {len(data_files_info['all_files'])} data files in {DATA_DIR}\\n\")\n",
    "    \n",
    "    print(\"Files by extension:\")\n",
    "    for ext, files in data_files_info['by_extension'].items():\n",
    "        print(f\"  {ext}: {len(files)} files\")\n",
    "    \n",
    "    print(\"\\nFiles by research area:\")\n",
    "    for area, files in data_files_info['by_area'].items():\n",
    "        if files:  # Only show areas with files\n",
    "            print(f\"  {area.replace('_', ' ').title()}: {len(files)} files\")\n",
    "else:\n",
    "    print(\"No data files found to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Functions\n",
    "\n",
    "Let's create functions to load different types of data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to load different data formats\n",
    "def load_data_file(file_path):\n",
    "    \"\"\"Load a data file based on its extension\"\"\"\n",
    "    ext = file_path.suffix.lower()\n",
    "    \n",
    "    try:\n",
    "        if ext == '.csv':\n",
    "            return pd.read_csv(file_path)\n",
    "        elif ext == '.json':\n",
    "            with open(file_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        elif ext == '.jsonl':\n",
    "            return pd.read_json(file_path, lines=True)\n",
    "        elif ext in ['.xlsx', '.xls']:\n",
    "            return pd.read_excel(file_path)\n",
    "        elif ext == '.parquet':\n",
    "            return pd.read_parquet(file_path)\n",
    "        elif ext == '.npy':\n",
    "            return np.load(file_path)\n",
    "        elif ext == '.txt':\n",
    "            with open(file_path, 'r') as f:\n",
    "                return f.readlines()\n",
    "        else:\n",
    "            print(f\"Warning: Unsupported file format {ext} for {file_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load data files by research area\n",
    "def load_area_data(area):\n",
    "    \"\"\"Load all data files for a specific research area\"\"\"\n",
    "    if not data_files_info or 'by_area' not in data_files_info:\n",
    "        return {}\n",
    "        \n",
    "    area_files = data_files_info['by_area'].get(area, [])\n",
    "    area_data = {}\n",
    "    \n",
    "    for file_path in area_files:\n",
    "        key = file_path.stem\n",
    "        data = load_data_file(file_path)\n",
    "        if data is not None:\n",
    "            area_data[key] = {\n",
    "                'path': file_path,\n",
    "                'data': data\n",
    "            }\n",
    "    \n",
    "    return area_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for each research area\n",
    "research_data = {}\n",
    "for area in RESEARCH_AREAS:\n",
    "    research_data[area] = load_area_data(area)\n",
    "    print(f\"Loaded {len(research_data[area])} datasets for {area.replace('_', ' ').title()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Completeness Analysis\n",
    "\n",
    "Let's analyze if we have data for all test modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_completeness():\n",
    "    \"\"\"Check if we have data for all test modules\"\"\"\n",
    "    completeness_results = {}\n",
    "    \n",
    "    for area in RESEARCH_AREAS:\n",
    "        test_modules = test_modules_by_area.get(area, [])\n",
    "        area_data_keys = list(research_data.get(area, {}).keys())\n",
    "        \n",
    "        # Check for each test module if we have corresponding data\n",
    "        module_coverage = {}\n",
    "        for module in test_modules:\n",
    "            module_name = module.replace('test_', '')\n",
    "            matching_keys = [k for k in area_data_keys if module_name.lower() in k.lower()]\n",
    "            \n",
    "            module_coverage[module] = {\n",
    "                'has_data': len(matching_keys) > 0,\n",
    "                'data_files': matching_keys\n",
    "            }\n",
    "        \n",
    "        # Check for data files that don't correspond to any test module\n",
    "        orphaned_data = []\n",
    "        for data_key in area_data_keys:\n",
    "            if not any(data_key.lower() in f\"{module.replace('test_', '')}_data\".lower() for module in test_modules):\n",
    "                orphaned_data.append(data_key)\n",
    "        \n",
    "        completeness_results[area] = {\n",
    "            'module_coverage': module_coverage,\n",
    "            'orphaned_data': orphaned_data,\n",
    "            'coverage_percentage': sum(1 for m in module_coverage.values() if m['has_data']) / max(1, len(module_coverage)) * 100,\n",
    "            'total_modules': len(module_coverage),\n",
    "            'modules_with_data': sum(1 for m in module_coverage.values() if m['has_data'])\n",
    "        }\n",
    "    \n",
    "    return completeness_results\n",
    "\n",
    "completeness_analysis = check_data_completeness()\n",
    "\n",
    "# Display completeness results\n",
    "print(\"Data Completeness Analysis:\\n\")\n",
    "\n",
    "for area, results in completeness_analysis.items():\n",
    "    print(f\"\\n{area.replace('_', ' ').title()}:\")\n",
    "    print(f\"  Module coverage: {results['coverage_percentage']:.1f}% ({results['modules_with_data']}/{results['total_modules']} modules have data)\")\n",
    "    \n",
    "    if results['orphaned_data']:\n",
    "        print(f\"  Orphaned data files (no matching test module): {len(results['orphaned_data'])}\")\n",
    "        for orphan in results['orphaned_data']:\n",
    "            print(f\"    - {orphan}\")\n",
    "    \n",
    "    print(\"  Module details:\")\n",
    "    for module, coverage in results['module_coverage'].items():\n",
    "        status = \"✅ Has data\" if coverage['has_data'] else \"❌ Missing data\"\n",
    "        print(f\"    - {module.replace('test_', '').replace('_', ' ').title()}: {status}\")\n",
    "        if coverage['has_data']:\n",
    "            for data_file in coverage['data_files']:\n",
    "                print(f\"      └─ {data_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Analysis\n",
    "\n",
    "Let's analyze the quality of the data by research area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_quality(data, name):\n",
    "    \"\"\"Analyze the quality of a dataset\"\"\"\n",
    "    results = {\n",
    "        'name': name,\n",
    "        'type': type(data).__name__,\n",
    "    }\n",
    "    \n",
    "    # For pandas DataFrame\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        results.update({\n",
    "            'rows': len(data),\n",
    "            'columns': len(data.columns),\n",
    "            'null_percentage': data.isnull().mean().mean() * 100,\n",
    "            'column_types': dict(data.dtypes.astype(str)),\n",
    "            'memory_usage': data.memory_usage(deep=True).sum() / (1024 * 1024),  # MB\n",
    "        })\n",
    "    \n",
    "    # For dictionaries (like JSON)\n",
    "    elif isinstance(data, dict):\n",
    "        results.update({\n",
    "            'keys': len(data),\n",
    "            'nested_keys': sum(1 for v in data.values() if isinstance(v, (dict, list))),\n",
    "        })\n",
    "        \n",
    "        # If all values are lists, count total entries\n",
    "        if all(isinstance(v, list) for v in data.values()):\n",
    "            results['total_entries'] = sum(len(v) for v in data.values())\n",
    "    \n",
    "    # For lists\n",
    "    elif isinstance(data, list):\n",
    "        results.update({\n",
    "            'entries': len(data),\n",
    "            'entry_type': type(data[0]).__name__ if data else 'unknown',\n",
    "        })\n",
    "    \n",
    "    # For numpy arrays\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        results.update({\n",
    "            'shape': data.shape,\n",
    "            'dtype': str(data.dtype),\n",
    "            'memory_usage': data.nbytes / (1024 * 1024),  # MB\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze quality for all datasets\n",
    "quality_analysis = {}\n",
    "\n",
    "for area in RESEARCH_AREAS:\n",
    "    area_data = research_data.get(area, {})\n",
    "    area_quality = []\n",
    "    \n",
    "    for dataset_name, dataset_info in area_data.items():\n",
    "        quality_results = analyze_dataset_quality(dataset_info['data'], dataset_name)\n",
    "        area_quality.append(quality_results)\n",
    "    \n",
    "    quality_analysis[area] = area_quality\n",
    "\n",
    "# Display quality analysis\n",
    "for area, quality_results in quality_analysis.items():\n",
    "    if not quality_results:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n{area.replace('_', ' ').title()} Data Quality Analysis:\")\n",
    "    for result in quality_results:\n",
    "        print(f\"\\n  Dataset: {result['name']}\")\n",
    "        print(f\"  Type: {result['type']}\")\n",
    "        \n",
    "        # Print type-specific metrics\n",
    "        if 'rows' in result:\n",
    "            print(f\"  Rows: {result['rows']:,}\")\n",
    "            print(f\"  Columns: {result['columns']}\")\n",
    "            print(f\"  Null percentage: {result['null_percentage']:.2f}%\")\n",
    "            print(f\"  Memory usage: {result['memory_usage']:.2f} MB\")\n",
    "        elif 'keys' in result:\n",
    "            print(f\"  Keys: {result['keys']}\")\n",
    "            print(f\"  Nested keys: {result['nested_keys']}\")\n",
    "            if 'total_entries' in result:\n",
    "                print(f\"  Total entries: {result['total_entries']:,}\")\n",
    "        elif 'entries' in result:\n",
    "            print(f\"  Entries: {result['entries']:,}\")\n",
    "            print(f\"  Entry type: {result['entry_type']}\")\n",
    "        elif 'shape' in result:\n",
    "            print(f\"  Shape: {result['shape']}\")\n",
    "            print(f\"  Data type: {result['dtype']}\")\n",
    "            print(f\"  Memory usage: {result['memory_usage']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Depth Analysis of Selected Datasets\n",
    "\n",
    "Let's perform more detailed analysis of selected datasets from each research area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dataset(data, name):\n",
    "    \"\"\"Create visualizations for a dataset\"\"\"\n",
    "    print(f\"\\nVisualization for: {name}\\n{'-'*80}\")\n",
    "    \n",
    "    # For pandas DataFrame\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        # Sample data preview\n",
    "        print(\"Sample data:\")\n",
    "        display(data.head())\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(\"\\nSummary statistics:\")\n",
    "        display(data.describe(include='all').T)\n",
    "        \n",
    "        # Missing values visualization\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        missing = data.isnull().mean().sort_values(ascending=False)\n",
    "        if any(missing > 0):\n",
    "            sns.barplot(x=missing.index, y=missing.values)\n",
    "            plt.title(f'Missing Values in {name}')\n",
    "            plt.xticks(rotation=90)\n",
    "            plt.ylabel('Fraction missing')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Select numeric columns for correlation analysis\n",
    "        numeric_data = data.select_dtypes(include=[np.number])\n",
    "        if numeric_data.shape[1] >= 2:\n",
    "            # Correlation heatmap\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            corr = numeric_data.corr()\n",
    "            mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "            sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', square=True)\n",
    "            plt.title(f'Correlation Matrix for {name}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Distribution of numeric fields\n",
    "            max_cols = min(5, len(numeric_data.columns))\n",
    "            selected_cols = numeric_data.columns[:max_cols]\n",
    "            plt.figure(figsize=(15, 3*max_cols))\n",
    "            for i, col in enumerate(selected_cols):\n",
    "                plt.subplot(max_cols, 1, i+1)\n",
    "                sns.histplot(numeric_data[col].dropna(), kde=True)\n",
    "                plt.title(f'Distribution of {col}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    # For dictionaries and lists\n",
    "    elif isinstance(data, (dict, list)):\n",
    "        # Convert to DataFrame if possible for visualization\n",
    "        try:\n",
    "            if isinstance(data, dict):\n",
    "                # Try to convert to DataFrame\n",
    "                if all(isinstance(v, list) for v in data.values()):\n",
    "                    # Check if all lists have the same length\n",
    "                    lengths = [len(v) for v in data.values()]\n",
    "                    if len(set(lengths)) == 1:\n",
    "                        df = pd.DataFrame(data)\n",
    "                        print(\"Converted dictionary to DataFrame:\")\n",
    "                        visualize_dataset(df, name)\n",
    "                        return\n",
    "                    else:\n",
    "                        print(\"Dictionary has lists of uneven lengths. Showing sample:\")\n",
    "                        for k, v in list(data.items())[:5]:\n",
    "                            print(f\"{k}: {v[:5]} (length: {len(v)})\")\n",
    "                else:\n",
    "                    # For nested dictionaries, show structure\n",
    "                    print(\"Dictionary structure:\")\n",
    "                    for k, v in list(data.items())[:5]:\n",
    "                        print(f\"{k}: {type(v).__name__} {'of length '+str(len(v)) if hasattr(v, '__len__') else ''}\")\n",
    "            elif isinstance(data, list):\n",
    "                # If list of dictionaries, convert to DataFrame\n",
    "                if all(isinstance(item, dict) for item in data[:10]):\n",
    "                    df = pd.DataFrame(data)\n",
    "                    print(\"Converted list of dictionaries to DataFrame:\")\n",
    "                    visualize_dataset(df, name)\n",
    "                    return\n",
    "                else:\n",
    "                    # Show sample of list\n",
    "                    print(f\"List of {len(data)} items. Sample:\")\n",
    "                    for item in data[:5]:\n",
    "                        print(f\"  {type(item).__name__}: {item}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not convert to DataFrame: {str(e)}\")\n",
    "            print(\"Sample data:\")\n",
    "            if isinstance(data, dict):\n",
    "                for k, v in list(data.items())[:5]:\n",
    "                    print(f\"{k}: {v}\")\n",
    "            else:  # list\n",
    "                for item in data[:5]:\n",
    "                    print(f\"  {item}\")\n",
    "    \n",
    "    # For numpy arrays\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        print(f\"Numpy array shape: {data.shape}, dtype: {data.dtype}\")\n",
    "        if len(data.shape) == 1 or (len(data.shape) == 2 and data.shape[1] < 20):\n",
    "            # For 1D arrays or 2D arrays with reasonable column count\n",
    "            try:\n",
    "                df = pd.DataFrame(data)\n",
    "                visualize_dataset(df, name)\n",
    "            except:\n",
    "                print(\"Could not convert numpy array to DataFrame.\")\n",
    "                print(f\"Sample:\\n{data[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one dataset from each area for detailed visualization\n",
    "for area in RESEARCH_AREAS:\n",
    "    area_data = research_data.get(area, {})\n",
    "    if not area_data:\n",
    "        print(f\"\\nNo datasets available for {area.replace('_', ' ').title()}\")\n",
    "        continue\n",
    "    \n",
    "    # Select the dataset with the most rows if DataFrame, or first dataset otherwise\n",
    "    selected_dataset = None\n",
    "    selected_name = None\n",
    "    max_rows = 0\n",
    "    \n",
    "    for name, info in area_data.items():\n",
    "        data = info['data']\n",
    "        if isinstance(data, pd.DataFrame) and len(data) > max_rows:\n",
    "            max_rows = len(data)\n",
    "            selected_dataset = data\n",
    "            selected_name = name\n",
    "    \n",
    "    if selected_dataset is None and area_data:\n",
    "        # If no DataFrames, just use the first dataset\n",
    "        selected_name = list(area_data.keys())[0]\n",
    "        selected_dataset = area_data[selected_name]['data']\n",
    "    \n",
    "    if selected_dataset is not None:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Detailed Analysis for {area.replace('_', ' ').title()}: {selected_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        visualize_dataset(selected_dataset, selected_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Consistency Analysis\n",
    "\n",
    "This section checks for consistency between related datasets and analyzes time series if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for time series data across datasets\n",
    "def analyze_time_series():\n",
    "    time_series_data = []\n",
    "    \n",
    "    # Look for DataFrames with timestamp/date columns\n",
    "    for area, area_data in research_data.items():\n",
    "        for name, info in area_data.items():\n",
    "            data = info['data']\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                # Check for datetime columns\n",
    "                date_cols = []\n",
    "                \n",
    "                # Look for columns with date/time in the name\n",
    "                for col in data.columns:\n",
    "                    col_lower = col.lower()\n",
    "                    if any(term in col_lower for term in ['time', 'date', 'timestamp', 'datetime']):\n",
    "                        date_cols.append(col)\n",
    "                \n",
    "                # Look for datetime dtypes\n",
    "                datetime_cols = data.select_dtypes(include=['datetime']).columns.tolist()\n",
    "                date_cols.extend([col for col in datetime_cols if col not in date_cols])\n",
    "                \n",
    "                if date_cols:\n",
    "                    time_series_data.append({\n",
    "                        'area': area,\n",
    "                        'name': name,\n",
    "                        'data': data,\n",
    "                        'date_columns': date_cols\n",
    "                    })\n",
    "    \n",
    "    return time_series_data\n",
    "\n",
    "time_series_datasets = analyze_time_series()\n",
    "\n",
    "# Visualize time series data if available\n",
    "if time_series_datasets:\n",
    "    print(f\"\\nFound {len(time_series_datasets)} datasets with time series data:\\n\")\n",
    "    \n",
    "    for ts_data in time_series_datasets:\n",
    "        print(f\"Dataset: {ts_data['name']} from {ts_data['area'].replace('_', ' ').title()}\")\n",
    "        print(f\"Time columns: {', '.join(ts_data['date_columns'])}\")\n",
    "        \n",
    "        # Try to convert the first time column to datetime\n",
    "        df = ts_data['data']\n",
    "        time_col = ts_data['date_columns'][0]\n",
    "        \n",
    "        try:\n",
    "            # Ensure the column is datetime type\n",
    "            if df[time_col].dtype != 'datetime64[ns]':\n",
    "                df[time_col] = pd.to_datetime(df[time_col], errors='coerce')\n",
    "            \n",
    "            # Time series analysis\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            if numeric_cols:\n",
    "                # Select up to 3 numeric columns to visualize\n",
    "                plot_cols = numeric_cols[:3]\n",
    "                plt.figure(figsize=(14, 7))\n",
    "                \n",
    "                # Sort by time column\n",
    "                df_sorted = df.sort_values(time_col)\n",
    "                \n",
    "                for col in plot_cols:\n",
    "                    plt.plot(df_sorted[time_col], df_sorted[col], label=col)\n",
    "                \n",
    "                plt.title(f'Time Series Analysis for {ts_data[\"name\"]}')\n",
    "                plt.xlabel(time_col)\n",
    "                plt.legend()\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Show basic statistics by time periods\n",
    "                print(\"\\nTime series statistics:\")\n",
    "                # Try to resample by day, week, or month depending on data density\n",
    "                date_range = (df_sorted[time_col].max() - df_sorted[time_col].min()).days\n",
    "                \n",
    "                if date_range > 90:  # More than 90 days\n",
    "                    freq = 'M'\n",
    "                    freq_name = 'month'\n",
    "                elif date_range > 14:  # More than 2 weeks\n",
    "                    freq = 'W'\n",
    "                    freq_name = 'week'\n",
    "                else:\n",
    "                    freq = 'D'\n",
    "                    freq_name = 'day'\n",
    "                \n",
    "                try:\n",
    "                    # Create time series index\n",
    "                    df_ts = df_sorted.set_index(time_col)\n",
    "                    # Resample and show count, mean, std\n",
    "                    resampled = df_ts[plot_cols].resample(freq).agg(['count', 'mean', 'std'])\n",
    "                    print(f\"Statistics by {freq_name}:\")\n",
    "                    display(resampled)\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not resample time series: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing time series: {str(e)}\")\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "else:\n",
    "    print(\"No time series datasets identified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Dataset Validation\n",
    "\n",
    "Check for relationships and consistency across datasets from the same research area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_related_datasets(area):\n",
    "    \"\"\"Find potentially related datasets in an area based on common column names\"\"\"\n",
    "    area_data = research_data.get(area, {})\n",
    "    df_datasets = {}\n",
    "    \n",
    "    # Collect all DataFrame datasets with their columns\n",
    "    for name, info in area_data.items():\n",
    "        data = info['data']\n",
    "        if isinstance(data, pd.DataFrame) and not data.empty:\n",
    "            df_datasets[name] = {\n",
    "                'data': data,\n",
    "                'columns': set(data.columns)\n",
    "            }\n",
    "    \n",
    "    # Find potential relationships between datasets\n",
    "    related_pairs = []\n",
    "    processed = set()\n",
    "    \n",
    "    for name1, info1 in df_datasets.items():\n",
    "        for name2, info2 in df_datasets.items():\n",
    "            if name1 == name2 or (name1, name2) in processed or (name2, name1) in processed:\n",
    "                continue\n",
    "                \n",
    "            processed.add((name1, name2))\n",
    "            \n",
    "            common_cols = info1['columns'].intersection(info2['columns'])\n",
    "            if len(common_cols) >= 1:  # At least one common column\n",
    "                related_pairs.append({\n",
    "                    'dataset1': name1,\n",
    "                    'dataset2': name2,\n",
    "                    'common_columns': common_cols,\n",
    "                    'similarity': len(common_cols) / min(len(info1['columns']), len(info2['columns']))\n",
    "                })\n",
    "    \n",
    "    # Sort by similarity\n",
    "    related_pairs.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    return related_pairs\n",
    "\n",
    "# Analyze relationships between datasets in each area\n",
    "for area in RESEARCH_AREAS:\n",
    "    related_pairs = find_related_datasets(area)\n",
    "    \n",
    "    if related_pairs:\n",
    "        print(f\"\\n{area.replace('_', ' ').title()} - Related Dataset Pairs:\")\n",
    "        for pair in related_pairs:\n",
    "            print(f\"  {pair['dataset1']} ↔ {pair['dataset2']}\")\n",
    "            print(f\"    Similarity: {pair['similarity']*100:.1f}%\")\n",
    "            print(f\"    Common columns: {', '.join(sorted(pair['common_columns']))}\")\n",
    "            \n",
    "            # Check if datasets can be joined\n",
    "            if len(pair['common_columns']) > 0:\n",
    "                data1 = research_data[area][pair['dataset1']]['data']\n",
    "                data2 = research_data[area][pair['dataset2']]['data']\n",
    "                \n",
    "                # Pick the first common column for join example\n",
    "                join_col = next(iter(pair['common_columns']))\n",
    "                \n",
    "                # Check for common values in the join column\n",
    "                common_values = set(data1[join_col].astype(str)) & set(data2[join_col].astype(str))\n",
    "                overlap = len(common_values) / max(1, min(data1[join_col].nunique(), data2[join_col].nunique()))\n",
    "                \n",
    "                print(f\"    Join analysis on '{join_col}':\")   \n",
    "                print(f\"      Dataset 1 unique values: {data1[join_col].nunique():,}\")\n",
    "                print(f\"      Dataset 2 unique values: {data2[join_col].nunique():,}\")\n",
    "                print(f\"      Common values: {len(common_values):,} (Overlap: {overlap*100:.1f}%)\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Tests and Validation\n",
    "\n",
    "Perform statistical tests to validate data correctness and detect anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def statistical_validation(area):\n",
    "    \"\"\"Perform statistical validations on datasets in the area\"\"\"\n",
    "    area_data = research_data.get(area, {})\n",
    "    results = []\n",
    "    \n",
    "    for name, info in area_data.items():\n",
    "        data = info['data']\n",
    "        if not isinstance(data, pd.DataFrame) or data.empty:\n",
    "            continue\n",
    "            \n",
    "        dataset_results = {'name': name, 'tests': []}\n",
    "        \n",
    "        # Select numeric columns for statistical tests\n",
    "        numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Run tests for each numeric column\n",
    "        for col in numeric_cols:\n",
    "            col_data = data[col].dropna()\n",
    "            if len(col_data) < 8:  # Need minimum sample size for tests\n",
    "                continue\n",
    "                \n",
    "            col_results = {'column': col}\n",
    "            \n",
    "            # 1. Check for normality (Shapiro-Wilk test)\n",
    "            try:\n",
    "                # Use smaller sample for Shapiro test to avoid excessive power\n",
    "                sample = col_data.sample(min(5000, len(col_data))).values\n",
    "                shapiro_stat, shapiro_p = stats.shapiro(sample)\n",
    "                col_results['normality'] = {\n",
    "                    'test': 'Shapiro-Wilk',\n",
    "                    'statistic': shapiro_stat,\n",
    "                    'p_value': shapiro_p,\n",
    "                    'is_normal': shapiro_p > 0.05\n",
    "                }\n",
    "            except Exception as e:\n",
    "                col_results['normality'] = {'error': str(e)}\n",
    "            \n",
    "            # 2. Check for outliers using IQR method\n",
    "            try:\n",
    "                q1 = col_data.quantile(0.25)\n",
    "                q3 = col_data.quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                lower_bound = q1 - 1.5 * iqr\n",
    "                upper_bound = q3 + 1.5 * iqr\n",
    "                outliers = col_data[(col_data < lower_bound) | (col_data > upper_bound)]\n",
    "                outlier_percent = len(outliers) / len(col_data) * 100\n",
    "                \n",
    "                col_results['outliers'] = {\n",
    "                    'count': len(outliers),\n",
    "                    'percentage': outlier_percent,\n",
    "                    'has_many_outliers': outlier_percent > 5  # Flag if >5% are outliers\n",
    "                }\n",
    "            except Exception as e:\n",
    "                col_results['outliers'] = {'error': str(e)}\n",
    "            \n",
    "            # 3. Autocorrelation for potential time series\n",
    "            if len(col_data) > 30:  # Need sufficient data points\n",
    "                try:\n",
    "                    # Calculate lag-1 autocorrelation\n",
    "                    autocorr = col_data.autocorr(lag=1)\n",
    "                    col_results['autocorrelation'] = {\n",
    "                        'lag_1': autocorr,\n",
    "                        'has_autocorrelation': abs(autocorr) > 0.3  # Moderate autocorrelation\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    col_results['autocorrelation'] = {'error': str(e)}\n",
    "            \n",
    "            dataset_results['tests'].append(col_results)\n",
    "        \n",
    "        results.append(dataset_results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run statistical validation for each area\n",
    "stats_results = {}\n",
    "for area in RESEARCH_AREAS:\n",
    "    stats_results[area] = statistical_validation(area)\n",
    "\n",
    "# Display statistical validation results\n",
    "for area, results in stats_results.items():\n",
    "    if not results:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n{area.replace('_', ' ').title()} - Statistical Validation Results:\")\n",
    "    \n",
    "    for dataset in results:\n",
    "        print(f\"\\n  Dataset: {dataset['name']}\")\n",
    "        \n",
    "        for test in dataset['tests']:\n",
    "            print(f\"    Column: {test['column']}\")\n",
    "            \n",
    "            if 'normality' in test and 'error' not in test['normality']:\n",
    "                norm = test['normality']\n",
    "                print(f\"      Normality: {'Normal' if norm['is_normal'] else 'Non-normal'} \"  \n",
    "                      f\"(p={norm['p_value']:.4f})\")\n",
    "            \n",
    "            if 'outliers' in test and 'error' not in test['outliers']:\n",
    "                out = test['outliers']\n",
    "                print(f\"      Outliers: {out['count']:,} ({out['percentage']:.2f}%)\" + \n",
    "                      (\" - MANY OUTLIERS\" if out.get('has_many_outliers', False) else \"\"))\n",
    "            \n",
    "            if 'autocorrelation' in test and 'error' not in test['autocorrelation']:\n",
    "                auto = test['autocorrelation']\n",
    "                print(f\"      Autocorrelation: {auto['lag_1']:.2f}\" + \n",
    "                      (\" - SIGNIFICANT\" if auto.get('has_autocorrelation', False) else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations\n",
    "\n",
    "This section summarizes our findings about the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_report():\n",
    "    \"\"\"Generate a summary report of the data validation\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATA VALIDATION SUMMARY REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Count datasets by area\n",
    "    datasets_by_area = {area: len(data) for area, data in research_data.items()}\n",
    "    total_datasets = sum(datasets_by_area.values())\n",
    "    \n",
    "    print(f\"\\nTotal datasets analyzed: {total_datasets}\")\n",
    "    for area, count in datasets_by_area.items():\n",
    "        if count > 0:\n",
    "            print(f\"  {area.replace('_', ' ').title()}: {count} datasets\")\n",
    "    \n",
    "    # Data completeness summary\n",
    "    print(\"\\nData Completeness:\")\n",
    "    for area, results in completeness_analysis.items():\n",
    "        if results['total_modules'] > 0:\n",
    "            print(f\"  {area.replace('_', ' ').title()}: {results['coverage_percentage']:.1f}% coverage \"  \n",
    "                  f\"({results['modules_with_data']}/{results['total_modules']} test modules have data)\")\n",
    "    \n",
    "    # Data quality issues\n",
    "    print(\"\\nData Quality Issues:\")\n",
    "    for area, results in stats_results.items():\n",
    "        issues = []\n",
    "        for dataset in results:\n",
    "            for test in dataset['tests']:\n",
    "                if 'outliers' in test and 'error' not in test['outliers'] and test['outliers'].get('has_many_outliers', False):\n",
    "                    issues.append(f\"High outliers in {dataset['name']} - {test['column']} ({test['outliers']['percentage']:.2f}%)\")\n",
    "                if 'normality' in test and 'error' not in test['normality'] and not test['normality']['is_normal']:\n",
    "                    issues.append(f\"Non-normal distribution in {dataset['name']} - {test['column']}\")\n",
    "        if issues:\n",
    "            print(f\"\\n  {area.replace('_', ' ').title()}:\")\n",
    "            for issue in issues[:5]:  # Show top 5 issues\n",
    "                print(f\"    - {issue}\")\n",
    "            if len(issues) > 5:\n",
    "                print(f\"    - ... and {len(issues) - 5} more issues\")\n",
    "        else:\n",
    "            print(f\"  {area.replace('_', ' ').title()}: No major issues detected\")\n",
    "            \n",
    "    # Recommendations\n",
    "    print(\"\\nRecommendations:\")\n",
    "    \n",
    "    # Check for missing data\n",
    "    missing_data_areas = []\n",
    "    for area, results in completeness_analysis.items():\n",
    "        if results['coverage_percentage'] < 100:\n",
    "            missing_data_areas.append(area)\n",
    "    \n",
    "    if missing_data_areas:\n",
    "        print(\"  1. Generate missing test data for modules:\")\n",
    "        for area in missing_data_areas:\n",
    "            missing_modules = [module for module, coverage in completeness_analysis[area]['module_coverage'].items() \n",
    "                             if not coverage['has_data']]\n",
    "            if missing_modules:\n",
    "                print(f\"     - {area.replace('_', ' ').title()}: {', '.join([m.replace('test_', '') for m in missing_modules])}\")\n",
    "    \n",
    "    # Check for time series datasets\n",
    "    if time_series_datasets:\n",
    "        print(\"  2. Consider time-based analysis for the following datasets:\")\n",
    "        for ts in time_series_datasets[:3]:  # Top 3\n",
    "            print(f\"     - {ts['name']} in {ts['area'].replace('_', ' ').title()}\")\n",
    "    \n",
    "    # Check for potential joins\n",
    "    has_joinable = False\n",
    "    print(\"  3. Explore relationships between datasets:\")\n",
    "    for area in RESEARCH_AREAS:\n",
    "        related_pairs = find_related_datasets(area)\n",
    "        if related_pairs:\n",
    "            has_joinable = True\n",
    "            top_pair = related_pairs[0]\n",
    "            print(f\"     - {area.replace('_', ' ').title()}: Join {top_pair['dataset1']} with {top_pair['dataset2']} on '{', '.join(sorted(top_pair['common_columns'])[:2])}'\")    \n",
    "\n",
    "    if not has_joinable:\n",
    "        print(\"     - No closely related datasets identified.\")\n",
    "\n",
    "generate_summary_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on the analysis, consider the following next steps:\n",
    "\n",
    "1. Address any data completeness issues by generating test data for modules without data\n",
    "2. Investigate statistical anomalies identified in the analysis\n",
    "3. Consider deeper analysis of time series data if relevant to your research\n",
    "4. Explore potential relationships between datasets through joins where appropriate\n",
    "5. Review test modules with low data coverage to ensure they are functioning correctly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
